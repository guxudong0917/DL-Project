# 卷积神经网络CNN

# 为什么需要 CNN

1. 传统的全连接神经网络，会面临参数过多的问题，比如1000个输入，100个输出，那权重层就有10^5个参数。
2. 解决了局部性问题：模仿人眼，人眼在观察图像时，一次只观察一小部分，在CNN中通过卷积核或叫卷积滤波器来解决，它像一盏聚光灯，不停地移动，提取每一个部分的特征
3. 它解决了平移不变性的问题：一个局部特征被平移到其他地方，由于卷积核相同，仍然能经过卷积+池化提取特征进入下一层。

# 卷积层（Convolution）

不同层词的卷积层学到的东西不一样，一般来说低层学到的是低级特征：边缘，纹理，随着层数的加深，卷积层提取到的特征也越来越高级。

### 输出尺寸计算
<img src="images/image.png" width="600">

一个卷积核会把局部感受野区域和卷积核一一相乘然后求和，对应一个输出元素，这样大大减少了参数。需要给定步幅stride，即应用卷积核的间隔，默认步幅为1。

当stride>1时，即跳过一些空间，用更少的空间位置表示原来更大的空间区域，这种方式叫下采用。（如果没用padding，不算下采样，因为这不是主动压缩信息） 如果直接这样算，那就会越算越小，而且边缘特征只能提取一次，所以引出padding填充的概念，即往输入图像的周围填充0，这样可以调整输出大小。

<img src="images/image 1.png" width="600">

输出大小计算公式：

OH=(H+2P-FH)/S +1

OW=(W+2P-FW)/S+1

可以这样理解：H+2P是填充后的图像，减去最开始的卷积核，剩下的除以S就是除去第一次要滑动多少次。】

### 多通道计算

一个卷积层可以用一个多通道的滤波器filter，每一个通道的kernel(2D卷积核)提取不同的输入，比如输入图片一般是RGB三通道，filter就对应有三个kernel，分别提取红，绿，蓝三种颜色的特征。

<img src="images/image 2.png" width="600">
 
可以看到输入的通道数要和卷积核(3D)的通道数相同，不同通道的卷积核大小应该相同，每个通道的运算结果相加，得到一个输出特征图。

要想得到多个特征图，可以用多个卷积核filter。 

<img src="images/image 3.png" width="600">

为什么需要多个滤波器呢,因为每一个滤波器可以提取不同的特征，比如颜色，方向，大小….

参数量计算：(卷积核长 × 卷积核宽 × 输入通道数) × 输出通道数 + bias

每个滤波器可以有一个偏置，bias形状为(FN，1，1).

# 池化层
 有Max池化层和Avg池化层，分别是取一块区域的最大值和平均值来作为输入。池化层可以用来下采样，只提取一个区域最主要的信息，用来减少参数量。

# 激活函数（ReLU）

一般来说，卷积神经网络中一个卷积层后会跟一个ReLu，这是为了引入非线性，否则和只有一层没有什么区别。用ReLu作为激活函数是因为它的反向传播非常简单，计算量小，而且可以避免梯度消失的问题。

## 一个完整 CNN 结构示例

### 卷积层（提取特征）：[Conv → ReLU] × N →Pooling→[Conv → ReLU] × M→Flatten
分类层：FC → 输出

早期的CNN是每层都池化的，但现代的CNN结构选择隔几层池化一次，因为这样可以减少信息的损失，每层都池化特征会消失的很快。